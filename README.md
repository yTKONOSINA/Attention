# Attention
Implementing Attention (or even possibly Transformer architecture) from scratch

# Tentative work plan

1. Implement Attention mechanism without any external libraries without automatic differentiation
2. Implement Transformer architecture without automatic differentiation 
3. Take some weights from Hugging Face and test the model
4. Show the heat maps