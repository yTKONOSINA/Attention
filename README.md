# Attention
Implementing Attention (or even possibly Transformer architecture) from scratch
